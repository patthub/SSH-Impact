# -*- coding: utf-8 -*-
"""Testy llm impact

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xi5Dq95IYQxWz_tXM6QzLn4Slr4GihRq
"""

#USTAWIENIA:
#Środowisko wykonawcze -> zmień typ środowiska wykonawczego -> T4

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install bitsandbytes accelerate xformers einops langchain faiss-cpu transformers sentence-transformers ipywidgets

from typing import List
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig
import torch
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain, RetrievalQA
from langchain.callbacks.tracers import ConsoleCallbackHandler
from langchain_core.vectorstores import VectorStoreRetriever
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline

import ipywidgets as widgets
from IPython.display import display, clear_output

import threading
import time

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Device:", device)
if device == 'cuda':
    print(torch.cuda.get_device_name(0))

orig_model_path = "mistralai/Mistral-7B-Instruct-v0.1"
model_path = "filipealmeida/Mistral-7B-Instruct-v0.1-sharded"
bnb_config = BitsAndBytesConfig(
                                load_in_4bit=True,
                                bnb_4bit_use_double_quant=True,
                                bnb_4bit_quant_type="nf4",
                                bnb_4bit_compute_dtype=torch.bfloat16,
                               )
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, quantization_config=bnb_config, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(orig_model_path)

text_generation_pipeline = transformers.pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    repetition_penalty=1.1,
    return_full_text=True,
    max_new_tokens=10000,
)
mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)



#Artykuł stanowi próbę dogłębnej analizy procesu tekstotwórczego opowiadania Zygmunta Haupta zatytułowanego "Entropia", wykorzystując bruliony utworu przechowywane w specjalnych zbiorach biblioteki Stanford University. Badanie to wpisuje się w kontekst obserwacji naukowców, którzy zwracają uwagę na złożoność literackich technik Haupta, skoncentrowanych na ukazywaniu skomplikowanego statusu podmiotu w świecie współczesnym, jak to miało miejsce w przypadku jego innego dzieła, "Lutnia". Autor artykułu dąży do udowodnienia, że bruliony Haupta stanowią integralną, żywą część jego twórczości, gdzie nigdy nie ustaje starcie o odkrywanie nowych wartości płynących z relacji między człowiekiem a miejscem. Przedstawiona analiza rzuca światło na głęboką strukturę narracyjną oraz ewolucję idei w twórczości tego wybitnego pisarza, podkreślając nieustającą aktualność tematów poruszanych w jego opowiadaniach."""

def format_response(text, max_line_length=80):
    words = text.split(' ')
    formatted_text = ''
    current_line = ''

    for word in words:
        if len(current_line) + len(word) + 1 <= max_line_length:
            current_line += word + ' '
        else:
            formatted_text += current_line + '\n'
            current_line = word + ' '

    formatted_text += current_line
    return formatted_text

def generate_response(abstract):
    progress_bar.value = 0.5
    prompt = f"{abstract}\n\nWhat kind of cultural, societal, or business impact can be stated for this article? Give me structured response in points: culture, business, society. Give me detailed response"
    response = mistral_llm.invoke(prompt)
    progress_bar.value = 1.0
    formatted_response = format_response(response)
    print("Response:\n", formatted_response)
    progress_bar.value = 0.0

prompt_input = widgets.Textarea(description="Paste abstract:")
submit_button = widgets.Button(description="Generate")
progress_bar = widgets.FloatProgress(value=0.0, min=0.0, max=1.0, description='Analyzing:')

def on_button_click(b):
    generate_response(prompt_input.value)

submit_button.on_click(on_button_click)

display(prompt_input, submit_button, progress_bar)